For Deliverable 5, we implemented 5 new optimizations, **Aggressive Dead Code Elimination** (ADCE), **Induction Variable Elimination** (IVS), **Loop Unroller** (LUP), **Merge Function** (MFP), and **Loop Rotation** (LRP). The first 3-4 implementations went relatively smoothly, with much trouble coming in for the last one. Many different optimiations were tried, such as jump threading, module inliner (even with the wrapper class), dead argument elimination, loop vectorization, and SCCP. Finally, we were able to find the loop rotation pass which finally worked after many different iterations of testing for it to show improvement. 

This work was built upon our previous work from Deliverables 1-4 rather than the base tipc compiler. 

For our methodology, we first generated a LLVM bitcode version of the program without the individual optimization applied. We would then generate another copy of the LLVM bitcode program with the new optimization applied, comparing how the structure of the bitcode changed (or didn't). The differences for each optimization will be shared in detail in its respective section below. Next, we would compile each of the two (with and without the new optimization) as an executable, comparing the execution times with the `time` command. Summarized results will be listed below with specifics being located in the respective `/sipc-abbot_lamb/test/optimization/` folder.

### Aggressive Dead Code Elimination (ADCE) ###

ADCE is a compiler optimization technique that removes code that has no effect on the observable behavior of the program, including unreachable code and computations whose results are never used. The example program that was used is as follows:

```bash
main(iters) {
  var a, b, c, d, e, f, g, i, j, unused1, unused2, unused3, unused4;
  a = 5;  
  b = 10; 
  c = 20; 
  d = 0;  
  e = 0;  
  f = 50;
  g = 75;
  unused1 = 100; 
  unused2 = 200; 
  unused3 = 300; 
  unused4 = 400; 
  for (i : 0 .. iters) {
    a = a + b; 
    b = b * 2; 
    unused1 = unused1 + c;
    unused2 = unused2 - d; 
    
    if (b > 100) {
      c = c + 10;
      unused3 = unused3 * 2; 
    } 
    else {
      d = d + 5;
      unused4 = unused4 / 3; 
    }

    for (j : 0 .. 5) {
      unused1 = unused1 + j;
      unused2 = unused2 - j;
      unused3 = unused3 * j;
      unused4 = unused4 / (j + 1);
    }

    e = e + a + b + c + d;

    if (f < g) {
      f = f + 1; 
    } 
    else {
      g = g - 1; 
    }
  }

  e = e + d;
  unused1 = unused1 + unused2 + unused3 + unused4;

  return e;
}
```

This program contains several variables, such as `unused1`-`4` that are calculated but never contribute to the program's return value `e`. These variables are "dead" because their values are unused in meaningful computation after assignment. The benefits from ACDE are as follows:

- Reduced LLVM IR code size - 103 lines of unoptimized code decreased to 74 lines of optimized code. Every mention of the `unused1`-`4` variables, which had no relevance to the return value, have been removed, including computations they are involved in. Additionally, branches and much of the control flow involving these variables is no longer present, making the bitcode much more efficient and succicint.
- Faster runtimes - The optimized version led to an execution that was ~3.5 times faster. Full results can be viewed in the `sipc-abbot_lamb/test/optimization/adce/result.txt` file.

The optimized version of our program, as generated by ADCE, demonstrates how irrelevant computations and variables can be systematically removed. This process enhances spatial and computational efficiency without altering the program's observable behavior.

### Induction Variable Elimination (IVS) ###

IVS was selected as a key optimization due to its ability to simplify loops by eliminating redundant calculations and improving the efficiency of loop execution. This optimization reduces the overhead of maintaining loop variables by using arithmetic operations to replace runtime computations with constants or simpler expressions when possible. The example program that was used is as follows:

```bash
main(iters) {
    var a, i, n;
    a = 37;
    i = 0;

    while (i < iters) {
        i++;
    }

    return a + i;
}
```

With IVS, the while loop is entirely removed, and the induction variable `i` is replaced with a precomputed value within the LLVM IR. The benefits are as follows:

- Reduced LLVM IR code size - 35 lines of unoptimized code decreased to 31 lines of optimized code. This code is much more succicint and easier to follow with these programmatically compiled values.
- Faster runtimes - The optimized version led to an execution that was 4 times faster. Full results can be viewed in the `sipc-abbot_lamb/test/optimization/ivs/result.txt` file. 

Although these numbers may seen relatively insignificant, a greater number of loop iterations and repetitive use of a program like this would have huge savings over the long term. It is interesting to look at, especially in this example, of how weirdly formatted the optimized bitcode is. Some branching occurs 100% of the time due to how the conditions are set as always true, showing the somewhat "erratic" nature and impact these passes can make and how subsequent passes could further improve optimization.

### Loop Unroller Pass (LUP) ###

Loop unrolling is an optimization technique where multiple iterations of a loop are combined into a single iteration, reducing the overhead of branch instructions and enabling further compiler optimizations. The example program that was used is as follows:

```bash
main() {
    var i, j, k, n, result;
    n = 1000;
    result = 0;
    i = 0;

    while (i < n) {
        j = 0;
        while (j < n) {
            k = 0;
            while (k < n) {
                result = result + (i * j * k); 
                k++;
            }
            j++;
        }
        i++;
    }

    return result;
}
```


The benefits of LUP are as follows:

- Faster runtimes - The optimized version led to an execution that was ~2 times faster. Full results can be viewed in the `sipc-abbot_lamb/test/optimization/lup/result.txt` file. One caveat with this microbenchmark is that no command line arguments can be given. It was found that this optimization provided no speedup when the variable `n` was replaced with an input variable `iters`.
- Reduced branching and simplified code structure - Due to the reorganization of the bitcode from this program, the control flow is broken up much more. For example, instead of iterating over 1 k value for each cycle, the optimized code now iterates over 12 k values at once. In addition, computations independent of the k are hoisted outside the loop. Due to this, less total iterations are made and branches jumped, making the parallelism of the program much greater.

With these upsides though comes a slight inherent downside: the size of the generated bitcode can increase a large amount. Nested loops, such as the one provided in the example, are especially prone to this. The LLVM bitcode file without LUP activated has 66 lines, while the optimized bitcode has 255. The spatial cost of having this optimization might be worth the execution speedup for some applications, but will vary case to case, especially when the number of iterations for these loops depends on a command line input.

### Merge Function Pass (MFP) ###

MFP is an optimization technique in LLVM that identifies and consolidates functions with identical or nearly identical logic into a single canonical implementation. This reduces code size and improves runtime efficiency by eliminating redundant code and improving cache performance. The example program that was used is as follows:

```bash
main() {
    var a, b;
    a = test1(100);
    b = test2(200);
    return 0;
}

test1(a) {
    var b;
    if (a == 10) {
        b = a * a + a / 100;
    }
    else {
        b = -13;
    }
    return b;
}

test2(a) {
    var b;
    if (a == 10) {
        b = a * a + a / 100;
    }
    else {
        b = -13;
    }
    return b;
}
```

In this code, functions `test1` and `test2` perform the exact same operations. With the optimized version, the work done inside of `test2` is replaced with a call to the `test1` function, reducing the number of lines of bitcode operations. If there was a greater amount of computation within this function, a lot more space would most likely be saved compared to the toy example we've provided. The benefits are as follows:

- Reduced code size - MFP was able to save a couple lines of bitcode during compilation (37 lines to 35), which would lead to large amounts of spatial saving following our explanation give above. Duplicate function implementations will also increase cache usage, so underlying effects from this poor program writing can be saved with MFP.

In theory, the two programs should run at least at the same runtime execution speed. However, looking at our `sipc-abbot_lamb/test/optimization/mfp/result.txt` file shows a slight difference between the two, with the optimized version being *slightly* slower. If a greater number of runs were made and an average was taken, the difference would most likely be insignificant. This slowdown is most likely due to the extra branch from the duplicate function to the one kept in order to keep computation. In addition, if additional passes were made after MFP, such as ADCE, the bitcode operations within the unused function would most likely be deleted. Even though this pass was the least promising and most likely least useful optimization that was added, it provided some interesting insight in how LLVM uses unconditional branching and willingly produces dead code.

<!-- ### Dead Argument Elimination (DAE) ###

DAE is an optimization pass in LLVM that aims to eliminate function arguments that are never used within the function. These unused arguments do not contribute to the computation, and removing them can reduce both the function's complexity and its memory footprint. DAE analyzes each function to determine which arguments are not used within the function body. If a function contains an argument that is never read or written to (i.e., it's not used in any expression or computation within the function), the argument is marked as "dead." The optimization then removes these dead arguments from the function's signature and modifies any call sites of the function to reflect the new signature without the unused arguments. The example program that was used is as follows:

```bash
main() {
    var result;
    result = test_dead_argument_elimination(10, 20);
    return result;
}

test_dead_argument_elimination(arg1, arg2) {
    var result, y;
    result = arg1 * 2;
    y = bar(5);
    return result;
}

bar(x) {
    return x * 3;
}
```

The benefits of DAE are:

- Slightly decreased code amount - Even though the number of lines of bitcode might not go down, the size/width of the lines might. This is because function parameters might be erased/discarded because they are not used.

During our testing, we found that the runtimes with and without the optimizations did not vary much due to this, as the optimization mainly focuses on the spatial impact on the program. This file can be located at `sipc-abbot_lamb/test/optimization/dae/dae.tip`. We thought that the inclusion of this is worthwhile as it can simplify code that is written by amateur-level programmers to avoid clutter and confusion when reviewing their bitcode generation in order to fix bugs. -->

### Loop Rotation Pass (LRP) ###

LRP was chosen as a key optimization because it improves performance by rearranging loops in a way that minimizes overhead and enhances cache efficiency. This optimization focuses on combining multiple independent loops that operate on the same data into a single loop, reducing the number of times the loop conditions are checked and improving the spatial locality of the data being accessed. By rotating the iterations, the optimization can reduce the number of loop control statements and improve instruction throughput. In programs with nested loops that perform similar operations, loop rotation helps reduce redundant operations and improves execution speed. The example program that was used is as follows:

```bash
foo(x) {
    var a, b, c, d, e, f, g;
    a = 0;
    b = 0;
    d = 0;
    e = 0;
    f = 0;

    while(c < 1000){
        if (c == 10) {
            c++;
        }
        c++;
        a = 0;
        while(a < 10){
            a++;
            b++;
            if (true) {
                a = a + b;
            }
            d = 0;
        }
    }

    return c;
}

main(iters) {
    var i, a;
    i = 0;
    a = 0;

    while(i < iters){
       a = a + foo(i);
       i++;
    }

    return a;
}
```

The key result of this optimization is the transformation of the loop structure into a simpler, more linear form, which reduces the number of loop headers, phi nodes, and unnecessary branching. As a result, the optimized LLVM bitcode IR is much cleaner to read with better control flow, better register usage, and improved execution efficiency. Due to these changes the complexity of the program's loop is decreased substantially. In addition, the following benefits are found:

- *Barely* reduced code size - LUP decreases the original 75 line LLVM bitcode file to 74 lines, which is barely an improvement; however, even though the spatial complexity of this program doesn't change much, the changes mentioned above are also noticed.
- Faster execution times - The optimized version led to an execution that was ~1.8 times faster. Full results can be viewed in the `sipc-abbot_lamb/test/optimization/lrp/result.txt` file. The performance gain will be better with greater number of iterations or in different use cases.

For these reasons, we believed that this optimization was useful and can help with complex matrix arithmetic in future SIP programs.

In the interest of full transparency, Chat-GPT was used to generate some of our example programs as Chat seemed to do a
better job at writing code which demonstrates the optimization passes than us.